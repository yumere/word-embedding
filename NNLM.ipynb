{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EXP_TITLE = \"adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "feature_size = 100\n",
    "hidden_unit = 500\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading & Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n_gram(words: list, n=window_size):\n",
    "    return [(words[i: i + n], words[i + n]) for i in range(len(words) - n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeab03ab3ff24c24b89446ab00ec2df4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "file_list = os.listdir(\"aclImdb/train/pos\")\n",
    "for filename in tqdm_notebook(file_list[:100], desc=\"File List\"):\n",
    "    with open(os.path.join(\"aclImdb/train/pos\", filename), \"rt\", encoding=\"utf-8\") as f:\n",
    "        data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a4cad8557e4637af2f18cf87cb72ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataSet Size:  27862\n",
      "Vocabulary Size:  4757\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "dataset = []\n",
    "\n",
    "for doc in tqdm_notebook(data, desc=\"Tokenizing\"):\n",
    "    words = nltk.word_tokenize(doc)\n",
    "    dataset += n_gram(words)\n",
    "    vocab.update(words)\n",
    "vocab = list(vocab)\n",
    "print(\"DataSet Size: \", len(dataset))\n",
    "print(\"Vocabulary Size: \", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting train, validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, train_size=0.7, test_size=0.3, random_state=100)\n",
    "train, valid = train_test_split(train, train_size=0.9, test_size=0.1, random_state=random.randint(0, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot encoding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(inputs: list or str):\n",
    "    if type(inputs) == str:\n",
    "        index = vocab.index(inputs)\n",
    "        r = np.zeros((len(vocab), 1))\n",
    "        r[index] = 1\n",
    "        return r\n",
    "    elif type(inputs) == list:\n",
    "        r = []\n",
    "        for w in inputs:\n",
    "            index = vocab.index(w)\n",
    "            zeros = np.zeros((len(vocab)))\n",
    "            zeros[index] = 1\n",
    "            r.append(zeros)\n",
    "        return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, (None, len(vocab)))\n",
    "Y = tf.placeholder(tf.float32, (len(vocab), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Projection_Layer\"):\n",
    "    proj_w = tf.get_variable(\"proj_w\", shape=(len(vocab), feature_size), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    proj_b = tf.get_variable(\"proj_b\", shape=(feature_size), initializer=tf.zeros_initializer())\n",
    "\n",
    "    projection_layer = tf.add(tf.matmul(X, proj_w), proj_b)\n",
    "\n",
    "    tf.summary.histogram(\"proj_w\", proj_w)\n",
    "    tf.summary.histogram(\"proj_b\", proj_b)\n",
    "    tf.summary.histogram(\"proj\", projection_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Hidden_Layer\"):\n",
    "    input_x = tf.reshape(projection_layer, (-1, 1))\n",
    "\n",
    "    hidden_w = tf.get_variable(\"hidden_w\", shape=(hidden_unit, window_size * feature_size), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    hidden_b = tf.get_variable(\"hidden_b\", shape=(window_size * feature_size))\n",
    "    hidden_layer = tf.tanh(tf.add(tf.matmul(hidden_w, input_x), hidden_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U = tf.get_variable(\"U\", shape=(len(vocab), hidden_unit))\n",
    "output = tf.tanh(tf.matmul(U, hidden_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", shape=(window_size * feature_size, 1), initializer=tf.contrib.layers.xavier_initializer())\n",
    "softmax_b = tf.get_variable(\"softmax_b\", shape=(1), initializer=tf.zeros_initializer())\n",
    "output = tf.add(tf.matmul(output, softmax_w), softmax_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'cost:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=output, dim=0))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "tf.summary.scalar(\"cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "train_writer = tf.summary.FileWriter(\"./logs/train_{}\".format(EXP_TITLE), graph=sess.graph)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499d35bdb0404428912cdcf2c0565eba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229454fd8ed147909f27e8b3b4f58017"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost:  8.51029\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "\n",
    "for epoch in tqdm_notebook(range(10), desc=\"Epoch\"):\n",
    "    for x, y in tqdm_notebook(train, leave=False):\n",
    "        _ = sess.run(train_op, feed_dict={X: one_hot(x), Y: one_hot(y)})\n",
    "        global_step += 1\n",
    "        if global_step % 500 == 0:\n",
    "            c, summary = sess.run([cost, merged], feed_dict={X: one_hot(x), Y: one_hot(y)})\n",
    "            tqdm.write(\"Cost: {}\".format(c))\n",
    "            train_writer.add_summary(summary, global_step)\n",
    "            try:\n",
    "                saver.save(sess, \"./models_{}/model_{}.ckpt\".format(EXP_TITLE, global_step))\n",
    "            except Exception:\n",
    "                os.mkdir(\"models_{}\".format(EXP_TITLE))\n",
    "                saver.save(sess, \"./models_{}/model_{}.ckpt\".format(EXP_TITLE, global_step))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
